# TPs-apprentissage-profond-dans-les-jeux

# TP1 :

## Objectif
L'objectif de ce TP est de se familiariser avec les outils essentiels du Reinforcement Learning (RL), notamment OpenAI Gym. Les √©tudiants vont explorer comment interagir avec un environnement RL et ex√©cuter des actions avant d'impl√©menter un algorithme d'apprentissage dans les s√©ances suivantes.

---

## A) Partie 1: Pr√©sentation des Biblioth√®ques Cl√©s

### 1. OpenAI Gym
**OpenAI Gym** est une biblioth√®que permettant de simuler des environnements interactifs pour tester des algorithmes de RL. Un environnement Gym est d√©fini par :
- Un ensemble **d'√©tats**
- Un ensemble **d'actions**
- Un **syst√®me de r√©compenses**
- Un crit√®re de **fin d‚Äô√©pisode**

### 2. Installation de Gym
```bash
pip install --upgrade gymnasium pygame numpy
```

### 3. Cr√©ation d'un environnement
```python
import gymnasium as gym

env = gym.make("CartPole-v1", render_mode="human")

env.reset()
```

---

## B) Partie 2: Exercices Pratiques avec OpenAI Gym

### Exercice 1: D√©couverte et Exploration d'un Environnement Gym
**Objectif** : Comprendre la structure d'un environnement Gym en explorant ses propri√©t√©s et ses actions possibles.

‚úî **Instructions** :
1. Afficher l'espace d'actions et l'espace d'observations.
2. Ex√©cuter une boucle de simulation avec des actions al√©atoires pendant 100 it√©rations.
3. Observer les valeurs des observations retourn√©es.

```python

print(f"Espace d'actions : {env.action_space}")
print(f"Espace d'observation : {env.observation_space}")

for _ in range(100):
    action = env.action_space.sample()
    observation, reward, done, _, _ = env.step(action)
    print(f"Action : {action}, Observation : {observation}, Reward : {reward}")
    if done :
        env.reset()

env.close

```

---

### Exercice 2: Manipulation des Observations et R√©compenses
**Objectif** : Comprendre comment r√©cup√©rer les observations et les r√©compenses lors de l'interaction avec l'environnement.

‚úî **Instructions** :
1. Prendre une action et r√©cup√©rer les valeurs retourn√©es (`observation`, `reward`, `done`).
2. Afficher ces valeurs et analyser leur signification.
3. Faire plusieurs essais et noter les variations des observations et des r√©compenses.

```python
import gymnasium as gym 

env = gym.make("CartPole-v1", render_mode="human")

num_steps = 10  
observation, _ = env.reset() 

for step in range(num_steps):
    action = env.action_space.sample() 
    new_observation, reward, done, _, _ = env.step(action)  

    print(f"√âtape {step + 1}:")
    print(f"Ancienne observation : {observation}")
    print(f"Action : {action}")
    print(f"Nouvelle observation : {new_observation}")
    print(f"R√©compense : {reward}, Termin√© ? {done}")
    print("-" * 40)

    observation = new_observation

    if done:
        print("L'√©pisode est termin√©. R√©initialisation de l'environnement.")
        observation, _ = env.reset()  

env.close()  

```

---

### Exercice 3: Contr√¥le Manuel de l'Agent
**Objectif** : Permettre √† l'utilisateur de contr√¥ler manuellement l'agent pour mieux comprendre l'effet des actions.

‚úî **Instructions** :
1. Demander √† l'utilisateur d'entrer une action (`0` ou `1`).
2. Ex√©cuter l'action dans l'environnement et afficher les nouvelles observations.
3. R√©p√©ter l'op√©ration jusqu'√† la fin de l'√©pisode.
4. Afficher la dur√©e totale de l'√©pisode avant qu'il ne se termine.

```python
import gymnasium as gym

env = gym.make("CartPole-v1", render_mode="human")
observation, _ = env.reset(seed=42)

done = False
step_count = 0

print("Contr√¥le manuel : entrez 0 (gauche) ou 1 (droite) pour d√©placer le chariot.")

while not done:
    env.render()

    action = input("Entrez votre action (0=gauche, 1=droite) : ")
    
    if action not in ["0", "1"]:
        print("Action invalide ! Entrez 0 ou 1.")
        continue
    
    action = int(action)

    observation, reward, done, _, _ = env.step(action)
    
    print(f"\n√âtape {step_count + 1}:")
    print(f"Action : {action}, Observation : {observation}, Reward : {reward}")
    print(f"  Termin√© ? {done}")
    print("-" * 40)

    step_count += 1

print(f"\nL'√©pisode est termin√© apr√®s {step_count} √©tapes.")

env.close()
```

---

### Exercice 4: √âvaluation des Performances d'une Politique Al√©atoire
**Objectif** : Mesurer la dur√©e moyenne d'un √©pisode lorsqu'un agent prend des actions al√©atoires.

‚úî **Instructions** :
1. Faire ex√©cuter √† l'agent des actions al√©atoires pendant plusieurs √©pisodes (ex: 10 √©pisodes).
2. Calculer la dur√©e moyenne avant que l'√©pisode ne se termine.
3. Comparer les r√©sultats entre plusieurs ex√©cutions.

```python
import gymnasium as gym
import numpy as np

env = gym.make("CartPole-v1", render_mode="human")

num_episodes = 10
episode_durations = []

for episode in range(num_episodes):
    observation, _ = env.reset()  
    done = False
    step_count = 0  

    while not done:
        action = env.action_space.sample() 
        observation, reward, done, _, _ = env.step(action)
        step_count += 1 

    episode_durations.append(step_count)  
    print(f"√âpisode {episode + 1} termin√© en {step_count} √©tapes.")


average_duration = np.mean(episode_durations)
std_duration = np.std(episode_durations) 
print("\nR√©sum√© des performances de la politique al√©atoire :")
print(f"  - Nombre d'√©pisodes : {num_episodes}")
print(f"  - Dur√©e moyenne : {average_duration:.2f} √©tapes")
print(f"  - √âcart-type : {std_duration:.2f}")

env.close() 
```
---
# TP2 :

### Exercice 1: Exploration de l'Environnement Frozen Lake
**Objectif** : Apprendre √† interagir avec un environnement Gym, explorer les actions possibles et comprendre les observations et r√©compenses retourn√©es par l'environnement.

‚úî **Instructions** :

1. Charger l'environnement FrozenLake-v1 de OpenAI Gym.
2. Afficher les informations de l'espace d'√©tats et d'actions.
3. Ex√©cuter une boucle o√π l'agent prend des actions al√©atoires pendant plusieurs √©pisodes.
4. Observer les observations et les r√©compenses obtenues.
```python
import gymnasium as gym
import numpy as np

env = gym.make("FrozenLake-v1", is_slippery=True)

print(f"Espace d'actions: {env.action_space}")
print(f"Espace d'observations: {env.observation_space}")

for _ in range(100):
    action = env.action_space.sample()
    observation, reward, done, _, _ = env.step(action)
    print(f"Action: {action}, Observation: {observation}, R√©compense: {reward}")
    if done:
        env.reset()

env.close()
```

### Exercice 2: Impl√©mentation de la Q-Table et Initialisation
**Objectif** : Cr√©er une Q-Table et la remplir d'initialisations avant d'apprendre.

‚úî **Instructions** :

1. Cr√©er une Q-Table de dimension (nombre d'√©tats x nombre d'actions), initialis√©e √† 0.
2. Afficher la Q-Table avant l'apprentissage.
3. V√©rifier que chaque √©tat a une liste de valeurs associ√©es aux actions possibles.
   
```python
import gymnasium as gym
import numpy as np

env = gym.make("FrozenLake-v1", is_slippery=True)

n_actions = env.action_space.n
n_states = env.observation_space.n
Q = np.zeros((n_states, n_actions))

print("Q-table avant apprentissage :")
print(Q)

```
### Exercice 3: Impl√©mentation du Q-Learning avec Mise √† Jour
**Objectif** : Impl√©menter l'algorithme Q-learning et mettre √† jour la Q-table √† chaque √©pisode.

‚úî **Instructions** :

1. D√©finir les hyperparam√®tres : taux d'apprentissage (alpha), facteur de discount (gamma), epsilon pour l'exploration.
2. Mettre √† jour la Q-table en appliquant la r√®gle de mise √† jour du Q-learning :
   Q(s,a)‚ÜêQ(s,a)+Œ±[R(s,a)+Œ≥ 
a 
‚Ä≤
 
max
‚Äã
 Q(s 
‚Ä≤
 ,a 
‚Ä≤
 )‚àíQ(s,a)]
3. Ex√©cuter plusieurs √©pisodes et observer l'√©volution de la table.
```python
import gymnasium as gym
import numpy as np

alpha = 0.1
gamma = 0.99
epsilon = 1.0
epsilon_decay = 0.995
num_episodes = 5000

env = gym.make("FrozenLake-v1", is_slippery=True)

n_actions = env.action_space.n
n_states = env.observation_space.n
Q = np.zeros((n_states, n_actions))

for episode in range(num_episodes):
    state, _ = env.reset()
    done = False
    while not done:
        if np.random.rand() < epsilon:
            action = env.action_space.sample()  
        else:
            action = np.argmax(Q[state])  
        
        next_state, reward, done, _, _ = env

```
### Exercice 4: √âvaluation du Q-Learning
**Objectif** : Tester la politique apprise en utilisant la Q-table apr√®s l'entra√Ænement et mesurer la performance de l'agent.

‚úî **Instructions** :

1. Lancer plusieurs √©pisodes en exploitant la politique apprise (choisir toujours l'action ayant la plus haute valeur Q).
2. Mesurer le taux de succ√®s de l'agent sur N √©pisodes (ex. 1000 √©pisodes).
3. Afficher les r√©sultats pour √©valuer si l'agent a bien appris.
   
```python
import gymnasium as gym
import numpy as np

num_eval_episodes = 1000
success_count = 0

env = gym.make("FrozenLake-v1", is_slippery=True)

for episode in range(num_eval_episodes):
    state, _ = env.reset()
    done = False

    while not done:
        action = np.argmax(Q[state])
        
        next_state, reward, done, _, _ = env.step(action)

        state = next_state

    if reward == 1.0:
        success_count += 1

success_rate = success_count / num_eval_episodes * 100
print(f"Taux de succ√®s apr√®s entra√Ænement : {success_rate:.2f}%")


```


---

## üìå Auteurs
- **Marouane ACHARIFI**  

---

üöÄ *Bon apprentissage du Reinforcement Learning !* üéØ

