# TPs-apprentissage-profond-dans-les-jeux


## Objectif
L'objectif de ce TP est de se familiariser avec les outils essentiels du Reinforcement Learning (RL), notamment OpenAI Gym. Les √©tudiants vont explorer comment interagir avec un environnement RL et ex√©cuter des actions avant d'impl√©menter un algorithme d'apprentissage dans les s√©ances suivantes.

---

## A) Partie 1: Pr√©sentation des Biblioth√®ques Cl√©s

### 1. OpenAI Gym
**OpenAI Gym** est une biblioth√®que permettant de simuler des environnements interactifs pour tester des algorithmes de RL. Un environnement Gym est d√©fini par :
- Un ensemble **d'√©tats**
- Un ensemble **d'actions**
- Un **syst√®me de r√©compenses**
- Un crit√®re de **fin d‚Äô√©pisode**

### 2. Installation de Gym
```bash
pip install --upgrade gymnasium pygame numpy
```

### 3. Cr√©ation d'un environnement
```python
import gymnasium as gym

# Cr√©ation de l'environnement CartPole-v1
env = gym.make("CartPole-v1", render_mode="human")

# R√©initialisation de l'environnement
env.reset()
```

---

## B) Partie 2: Exercices Pratiques avec OpenAI Gym

### Exercice 1: D√©couverte et Exploration d'un Environnement Gym
**Objectif** : Comprendre la structure d'un environnement Gym en explorant ses propri√©t√©s et ses actions possibles.

‚úî **Instructions** :
1. Afficher l'espace d'actions et l'espace d'observations.
2. Ex√©cuter une boucle de simulation avec des actions al√©atoires pendant 100 it√©rations.
3. Observer les valeurs des observations retourn√©es.

```python
import gymnasium as gym

env = gym.make("CartPole-v1", render_mode="human")
print(f"Espace d'actions: {env.action_space}")
print(f"Espace d'observations: {env.observation_space}")

for _ in range(100):
    action = env.action_space.sample()
    observation, reward, done, _, _ = env.step(action)
    print(f"Action: {action}, Observation: {observation}, Reward: {reward}")
    if done:
        env.reset()

env.close()
```

---

### Exercice 2: Manipulation des Observations et R√©compenses
**Objectif** : Comprendre comment r√©cup√©rer les observations et les r√©compenses lors de l'interaction avec l'environnement.

‚úî **Instructions** :
1. Prendre une action et r√©cup√©rer les valeurs retourn√©es (`observation`, `reward`, `done`).
2. Afficher ces valeurs et analyser leur signification.
3. Faire plusieurs essais et noter les variations des observations et des r√©compenses.

```python
import gymnasium as gym 

env = gym.make("CartPole-v1", render_mode="human")

num_steps = 10  
observation, _ = env.reset() 

for step in range(num_steps):
    action = env.action_space.sample() 
    new_observation, reward, done, _, _ = env.step(action)  

    print(f"√âtape {step + 1}:")
    print(f"Ancienne observation : {observation}")
    print(f"Action : {action}")
    print(f"Nouvelle observation : {new_observation}")
    print(f"R√©compense : {reward}, Termin√© ? {done}")
    print("-" * 40)

    observation = new_observation

    if done:
        print("L'√©pisode est termin√©. R√©initialisation de l'environnement.")
        observation, _ = env.reset()  

env.close()  
```

---

### Exercice 3: Contr√¥le Manuel de l'Agent
**Objectif** : Permettre √† l'utilisateur de contr√¥ler manuellement l'agent pour mieux comprendre l'effet des actions.

‚úî **Instructions** :
1. Demander √† l'utilisateur d'entrer une action (`0` ou `1`).
2. Ex√©cuter l'action dans l'environnement et afficher les nouvelles observations.
3. R√©p√©ter l'op√©ration jusqu'√† la fin de l'√©pisode.
4. Afficher la dur√©e totale de l'√©pisode avant qu'il ne se termine.

```python
import gymnasium as gym

env = gym.make("CartPole-v1", render_mode="human")
observation, _ = env.reset()
done = False
step_count = 0

while not done:
    action = input("Entrez votre action (0=gauche, 1=droite) : ")
    if action not in ["0", "1"]:
        print("Action invalide ! Entrez 0 ou 1.")
        continue
    
    action = int(action)
    observation, reward, done, _, _ = env.step(action)
    step_count += 1
    print(f"√âtape {step_count} - Observation: {observation}, R√©compense: {reward}, Termin√©: {done}")

env.close()
print(f"L'√©pisode s'est termin√© apr√®s {step_count} √©tapes.")
```

---

### Exercice 4: √âvaluation des Performances d'une Politique Al√©atoire
**Objectif** : Mesurer la dur√©e moyenne d'un √©pisode lorsqu'un agent prend des actions al√©atoires.

‚úî **Instructions** :
1. Faire ex√©cuter √† l'agent des actions al√©atoires pendant plusieurs √©pisodes (ex: 10 √©pisodes).
2. Calculer la dur√©e moyenne avant que l'√©pisode ne se termine.
3. Comparer les r√©sultats entre plusieurs ex√©cutions.

```python
import gymnasium as gym
import numpy as np

env = gym.make("CartPole-v1", render_mode="human")
num_episodes = 10

durations = []
for episode in range(num_episodes):
    observation, _ = env.reset()
    done = False
    step_count = 0
    while not done:
        action = env.action_space.sample()
        observation, reward, done, _, _ = env.step(action)
        step_count += 1
    durations.append(step_count)
    print(f"√âpisode {episode + 1} termin√© en {step_count} √©tapes.")

print(f"Dur√©e moyenne des √©pisodes : {np.mean(durations):.2f} √©tapes")
env.close()
```

---

## Conclusion
Ce TP permet une premi√®re immersion dans le Reinforcement Learning avec OpenAI Gym. Les exercices montrent comment interagir avec un environnement RL et comprendre les concepts cl√©s (observations, actions, r√©compenses). Dans les prochaines sessions, nous impl√©menterons des algorithmes d‚Äôapprentissage comme **Q-Learning** et **Deep Q-Networks (DQN)** pour am√©liorer la performance de l'agent. üöÄ

---

## üìå Auteurs
- **Marouane ACHARIFI**  

---

üöÄ *Bon apprentissage du Reinforcement Learning !* üéØ

